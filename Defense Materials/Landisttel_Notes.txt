I think the motivation for using the treelet transformation approach could be improved. The opening line of section 1.3, stating that “High-dimensional data analysis includes settings of statistical learning when using data sets with a high number of covariates (or inputs, features, etc.), often paired with a comparatively small number of observations” is not really applicable here since the sample size is very large even compared to the large number of codes. This setting, of having similarities in different diagnosis codes with sparse data, is very different from the other cases you mention, like imaging and omics data, which are often highly correlated measurements where PCA usually works well to reduce the number of variables but retain a high percentage of the variability. The treelet transformation seems to be for a different setting specific to sparse data. I kind of get the idea of that but I think you’re mixing together different types of data reduction methods without a clear delineation of the need for this method. Some examples from the literature with other uses of treelet transformations may be helpful to better justify the motivation.

Looking at Table 3 in the results, it is unclear to me how much of the prognostic accuracy is already captured in the clinical features and how much is added through the variable(s) generated with this new method. How many variables are being added to the logistic model and why not show them in a table similar to table 3? Later I saw it seems to be 133 clusters, but that was not evident to me until I got to the Appendix. Maybe it was there and I just missed it, but, in any case, I think it could be clarified further.

I’m also a little unclear on the very basic question of whether this is a supervised or unsupervised method. My initial understanding from section 2.2.1 was that this is an unsupervised method, such as PCA, since I do not see the outcome measure being used in the calculations. However, since there seems to be a separate generation of the treelet transformation for each outcome, with use of cross-validation to get the different parameter values – so it then seemed that it is actually a supervised method. Or, are the treelet features the same across outcomes (e.g. it is unsupervised) but you are showing their significance of association with each outcome in each section. If the last case is correct, I would suggestion have a separate section at the beginning, before going into the different outcomes, that describes the unsupervised results.

I am not clear whether the density curves of predicted probabilities are from the GLM using the treelet transformation as an additional feature(s) or from the treelet method along. If this is from the GLM with individual variables + the treelet features, I think you should show separate plots for the GLM of individual features along then the plot where the treelet features are added.

I think you should show the ROC curves. I would again show separate curves for the GLM with and without the treelet features. I think it is important for the reader to better see how much is being gained with this method. And – if the treelet features to lead to substantial gains, could you obtain similar results by just using a few of the most significant diagnostic codes?

For Figure 12, why only show this for one outcome? Is it the distribution of LOS and something specific about the negative binomial model that necessitates this figure in a way that is not applicable to other methods and outcomes?
 